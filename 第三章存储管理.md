# 第三章存储管理
在pgsql中，有专门的模块负责管理存储设备，称之为存储管理器。存储管理器提供了一组统一的管理外存与内存资源的功能模块，所有对外存与内存的操作都将交由存储管理器处理，可以认为存储管理器是数据库管理系统与物理存取设备的接口。与pgsql其他模块相比，存储管理器处在系统结构的底层，包含了操作物理存取设备的接口。

## 3.1 存储器管理的体系结构
&ensp;pgsql的存储管理主要包括两个功能：内存管理和外存管理。除了管理内存和外存的交互外，存储管理器的另一个主要任务是对内存进行统筹安排和规划。存储管理器体系结构如下图所示。
![memorymanagesystem.jpg](https://s1.ax1x.com/2020/06/19/NueRDP.jpg)
&ensp;内存管理包括共享内存的管理以及进程本地内存的管理。在共享内存中存储着所有进程的公共数据，例如锁变量，进程通信、缓冲区等。本地内存为每个后台进程所专有，是它们的工作区域，存储着属于该进程的Cache、事务管理信息、进程信息等。pgsql提供了轻量级锁，用于支持对共享内存中统一数据的互斥访问。存储管理器还提供了内存上下文(memorycontext)用于统一管理内存的分配和回收，从而更加有效安全地对内存进行管理。</br>
&ensp;外存管理包括表文件管理、空闲文件管理、虚拟文件描述符管理及大数据存储管理等。在pgsql中，**每一个表都用一个文件（表文件）存储**，表文件以表的oid命名。超出操作系统文件大小限制的表，pgsql会自动切成多个文件来存储，并自动按顺序标号标识它们。每个表除了表文件以外还有两个附属文件：可见性映射表文件(VM)和空闲空间映射表(FSM)文件。前者用于加快清理操作(VACUUM)的执行速度，后者用于表文件空闲空间的管理。</br>
&ensp;pgsql采用了份也存储管理方式，数据在内存中是以页面块的形式存在。每个表文件由多个BLCKSZ(可配置的常量)字节大小的文件块组成，每个文件块又可以包含多个元组。表文件以文件块为单位读入内存中，每一个文件块在内存中形成一个页面块。页面块是文件块在内存中的存在形式，后文也用页面来指代文件块。一个文件块可以存放多个元组，但是pgsql不支持元组的跨块存储，每个元组最大为MaxHeapTupleSize。这样保证了每个文件块存储的是多个完整的元组（思考，会不会造成存储区域的浪费）</br>
&ensp;pgsql在内存中开辟了缓冲区域用于存储这些文件块，我们将其在内存中开辟的缓冲区称为缓冲池，缓冲池被划分为若干个固定大小的缓冲区，磁盘上的文件块在读入内存后被存放在缓冲区中，称之为页面块或者缓冲块。BLCKSZ的默认值是8192，因此一个标准缓冲块的默认大小也是8KB。</br>

总体来说，存储管理器的主要任务包括：
> 1. 缓冲池管理：缓冲池在pgsql中起缓存作用。数据库中的事务常常需要频繁地存取数据，为减少对磁盘的读写，在事务执行时，数据首先将会放入缓冲池中，pgsql设立了进程间共享的缓冲池以及进程私有的缓冲池（本地缓冲池）
> 2. cache机制：将进程最近使用的一些是系统数据缓存在私有内存中，其级别高于缓冲池。
> 3. 虚拟文件描述符：pgsql通过虚拟文件描述符(virtual file descriptor,VFD)来对物理文件进行管理，避免因为操作系统对进程打开文件数的限制出现错误。
> 4. 空闲空间管理：用于快速定位到表文件中的空闲空间以便于插入新数据，从而提高空间利用率。
> 5. 进程间通信机制（IPC）：pgsq是一个多进程的系统，IPC用来在多个后台进程间进行通信和消息的传递，比如使用消息队列来同步进程产生的无效消息，同时IPC还提供了对共享内存的管理。
> 6. 大数据存储管理：提供大对象和TOAST机制。大对象机制是一个由用户控制的大数据存储方法。它允许用户调用函数，通过SQL语句直接向表中插入一个大尺寸文件（图片、视频等）。而TOAST机制是在用户插入的变长数据超过一定限制后自动触发，用户无法对TOAST加以控制。

&ensp;当一个pgsql进程从数据库读写一个元组的时候，可以用下图来描述。
![rwtuple.jpg](https://s1.ax1x.com/2020/06/19/NutkSx.jpg)
> 1. 读取一个元组数据时，首先获取表的基本信息（oid,索引统计等）及元组的模式信息，这些信息被记录在多个系统表中。通常表模式表化率很低，为了减少对系统表的访问，进程的本地内存区域设置了两种cache，一种用来存储系统表元组，一种用来存储表的基本信息。
> 2. 接下来要读区元组所在的文件块获得元组的数据。pgsql进程将首先从共享缓冲池中查找所需文件对应的缓冲块。如果找到就直接从中取得元组的数据并配合上一步得到的模式信息解析出元组的各个属性值。如果找不到，读入文件块，从被调入的缓冲块中获取元组数据。本地缓冲池用于缓存临时表数据及其它进程不可见数据。
> 3. 如果缓冲池没有包含所需元组，则要从存储介质中获取，并写入到缓冲区中。缓冲区与存储介质的交互是通过存储介质管理器(SMGR)进行的。不同存储介质的底层实现有差异，SMGR负责通关各种介质，并对上层的请求提供统一的接口，对下层则根据不同介质调用相应的介质管理器，因此SMGR是pgsql外存管理部分的核心。不仅支持对磁盘的管理，磁盘管理器负责管理所有存储在磁盘上的文件的操作，包括创建、删除、读取等。
> 4. 虚拟文件描述符机制(VFD)是为了防止程序打开的文件超过系统的限制引起的位置错误。
> 5. 在写元组时，需要找到一个合适空闲空间的缓冲块，然后填入缓冲块。如果便利所有空闲空间，效率很低，因此pgsql为每个表增加了一个附属文件，即空闲空间映射表(FSM)，用于记录每个表文件块的空闲空间大小。
> 6. pgsql使用表计删除的方式处理元组的删除操作，并非从物理上删除，而清楚工作由VACUUM机制来完成。遍历所有文件块查找被删除的元组效率很低。因此设置了可见性映射表(VM)加快查找速度。
> 7. 大数据的存储使用TOASt机制和大对象机制来实现，前者主要用于变长字符串，后者主要用于大尺寸的文件。写入元组时，如果超过大小限制，会自动触发TOAST机制对该元组进行相应处理。大对象机制则由用户来调用，当用户需要在数据库中存放一个大文件时，可以调用pgsql提供的接口函数创建一个大对象并获得oid，用户通过大对象oid来存储并使用大对象。

## 3.2 外存管理
&ensp;外存管理负责处理数据库与外存介质的交互过程。外存管理由SMGR(主要代码在[smgr.c](https://github.com/zhizhengyang/postgresql/blob/a4cae1310bbcfc298c601e5495a1a45203088889/src/backend/storage/smgr/smgr.c)中)提供对外存操作的统一接口，其结构如图所示。
![rommanagesystem.jpg](https://s1.ax1x.com/2020/06/19/NK9WCt.jpg)
&ensp;每个表文件在磁盘中都以一定的结构进行存储，针对磁盘，外存管理模块提供了磁盘管理器和VFD机制（虚拟文件描述符）
### 3.2.1 表和元组的组织方式
&ensp;在pgsql中，同一个表中的元组按照创建顺序依次插入到表文件中。元组之间不进行关联，这样的表文件称为堆文件。pgsql系统中包含了四种堆文件：普通堆（ordinary cataloged heap）、临时堆(temporary heap)、序列（SEQUENCE relation，一种特殊的单行表）和TOAST表。前面描述的堆文件就是普通堆。临时堆结构与普通堆相同，但临时堆尽在会话过程中临时创建，会话结束后会自动删除。序列则是一种元组自动增长的特殊堆。TOAST表其实也是一种普通堆，但是它被专门用于存储变长数据。尽管这几种堆文件功能各异，但在底层的文件结构却是相似的：每个堆文件都是由多个文件块组成，在物理磁盘中的存储如下图所示
![stackfilestory.jpg](https://s1.ax1x.com/2020/06/19/NKiZHe.jpg)
其中:
1. PageHeaderData是长度为20字节的页头数据，包含
- 空闲空间的起始和结束为止
- Special space的开始位置
- 项指针
- 标志信息，如是否存在空闲项指针、是否所有的元组都可见
2. Linp是ItemIdData类型的书组，ItemIdData类型由lp_off、lp_flags和lp_len三个属性组成。每一个ItemIdData结构用来指向文件块的一个元组，其中lp_off是元组在文件块的偏移量，而lp_len则说明了该元组的长度，lp_flags表示元组的状态（未使用、正常使用、HOT重定向和死亡四种状态）。每个Linp数组元素的长度为4字节，Linp1指向tuple1，Linp2指向tuple2
3. Freespace是指未分配的空间，新插入页面中的元组及其对应的Linp元素都将从Freespace的开头开始分配，而新元组数据则从尾部开始分配。
4. Special space是特殊空间，用于存放与索引方法相关的特定数据，不同的索引方法在Special space中存放不同的数据。由于索引文件的文件块结构和普通表文件的相同，因此Special space在普通表文件块中并没有使用，其内容被置为空。
&ensp;元组信息中还存放了元组头部信息，该信息通过HeapTupleHeaderData结构描述
```
typedef struct HeapTupleHeaderData
{
	union
	{
		HeapTupleFields t_heap;/*用于记录对元组执行插入/删除操作的事务ID和命令ID，
                         用于并发控制中对事务的可见性*/

		DatumTupleFields t_datum;//记录元组的长度信息
	}			t_choice;
	ItemPointerData t_ctid;		/* 记录当前元组或者新元组的物理位置 */

	/* Fields below here must match MinimalTupleData! */

	uint16		t_infomask2;	/* 低11位表示当前元组的属性个数，其他位用于HOT技术及
                              元组可见性的标志位 */

	uint16		t_infomask;		/* 标识元组当前的状态 */

	uint8		t_hoff;			/* 元组头的大小 */

	/* ^ - 23 bytes - ^ */

	bits8		t_bits[1];		/* 记录元组中哪些字段为空*/

	/* MORE DATA FOLLOWS AT END OF STRUCT */
} HeapTupleHeaderData;	
```
### 3.2.2 磁盘管理器
&ensp;磁盘管理器是SMGR的一种具体实现，它对外提供了管理磁盘介质的接口，其主要是现在[md.c](https://github.com/zhizhengyang/postgresql/blob/a4cae1310bbcfc298c601e5495a1a45203088889/src/backend/storage/smgr/md.c)中。磁盘管理器并非对磁盘上的文件直接进行操作，而是通过VFD机制来进行文件操作。
磁盘文件描述符结构如下

```
typedef struct _MdfdVec
{
	File		mdfd_vfd;		/* 该磁盘文件对应的VFD编号*/
	BlockNumber mdfd_segno;		/* 该磁盘文件对应的表文件段号 */
	struct _MdfdVec *mdfd_chain;	/* n指向同一个表文件的下一段 */
} MdfdVec;
```

VFD的具体实现在[fd.c](https://github.com/zhizhengyang/postgresql/blob/master/src/backend/storage/file/fd.c)中
### 3.2.3 VFD机制
#### 1.VFD实现原理
&ensp;VFD机制的原理类似连接池，当进程申请打开一个文件时，总能返回一个虚拟文件描述符，对外封装了打开物理文件的实际操作。VFD结构中记录了操作系统为文件分配的真实文件描述符。由于进程使用了VFD，它自身会觉得可以打开任意多个文件，不再受操作系统的限制。同一个文件对于不同进程而言，其VFD可能是不同的。VFD、真实文件描述符、文件之间的关系如下图所示。
![vfdfdfilerelation.jpg](https://s1.ax1x.com/2020/06/19/NK8huD.jpg)

VFD结构如下
```
typedef struct vfd
{
	int			fd;				/* current FD, or VFD_CLOSED if none */
	unsigned short fdstate;		/* bitflags for VFD's state */
	SubTransactionId create_subid;		/* for TEMPORARY fds, creating subxact */
	File		nextFree;		/* link to next free VFD, if in freelist */
	File		lruMoreRecently;	/* doubly linked recency-of-use list */
	File		lruLessRecently;
	off_t		seekPos;		/* current logical file position */
	char	   *fileName;		/* name of file, or NULL for unused VFD */
	/* NB: fileName is malloc'd, and must be free'd when closing the VFD */
	int			fileFlags;		/* open(2) flags for (re)opening the file */
	int			fileMode;		/* mode to pass to open(2) */
} Vfd;
```
#### 2.LRU池
&ensp;在每一个pgsql后台进程中都使用一个LRU(最近最少使用)来管理所有已经打开的VFD，池中每个VFD都对应一个物理上已经打开的文件。每一个进程都拥有其私有的LRU池和一些列的VFD，进程需要打开文件时都是从自己死后的LRU池中申请VFD。</br>
&ensp;当LRU池未满时，进程打开的文件个数未超过系统限制时，进程可以申请一个VFD来打开一个物理文件，当LRU池已经满的时候，进程需要先关闭一个VFD，这样打开新的文件时就不会因为超出系统限制造成不可预料的错误。
### 3.4 空闲空间映射表
&ensp;对于每个表文件，创建一个名为“关系表OID_fsm（例oid为12000，则表明为12000_fsm）”的文件，记录该表的空闲空间大小，称之为空闲空间映射表文件。</br>
&ensp;为了可以快速查找fsm文件，fsm文件不应太大，用一个字节来记录空闲空间范围

| 字节取值  | 表示的空闲空间范围  |
|:----------|:----------|
| 0    | 0～31    |
| 1    | 32～63    |
| ～   | ～～  |
| 255   | 8164～8192  |

&ensp;若表块没有空闲空间，则用0表示，对于有N字节空闲空间表块，在FSM中记录值为（31+N）/32向上取整。</br>
&ensp;为了快速查找，FSM文件使用了一个最大堆二叉树结构，第0层和第1层只用于查找，不记录值，叶子结点从左至右顺序对应表文件中的文件块。
&ensp;每个FSM块大小默认为8KB，大部分都储存块内的二叉树结构，大概可以保存4000个叶子结点。一个FSM文件总共可以记录4000^3个叶子结点（表块），远大于单个表的最大块数（2^32）
![fsmpyhysicalstory.jpg](https://s1.ax1x.com/2020/06/20/NlQD2V.jpg)

下面描述FSM的数据结构
```
typedef struct
{
	/*
	 * fsm_search_avail() tries to spread the load of multiple backends by
	 * returning different pages to different backends in a round-robin
	 * fashion. fp_next_slot points to the next slot to be returned (assuming
	 * there's enough space on it for the request). It's defined as an int,
	 * because it's updated without an exclusive lock. uint16 would be more
	 * appropriate, but int is more likely to be atomically
	 * fetchable/storable.
	 */
	int			fp_next_slot;//用于提示下一次开始查询二叉树的叶子结点位置	

	/*
	 * fp_nodes contains the binary tree, stored in array. The first
	 * NonLeafNodesPerPage elements are upper nodes, and the following
	 * LeafNodesPerPage elements are leaf nodes. Unused nodes are zero.
	 */
	uint8		fp_nodes[1];//存储了一个最大堆二叉树
} FSMPageData;
```

#### 1. 创建FSM
&ensp;创建fsm时，会预先创建三个FSM块：第0号为根结点即第0层的FSM块，第1号为第一层的第一个结点，第2号为第2层的第一个结点。当第2号FSM块满了以后，会扩展新的FSM块,在[freespace.c](https://github.com/zhizhengyang/postgresql/blob/master/src/backend/storage/freespace/freespace.c)文件中的[fsm_extend](https://github.com/zhizhengyang/postgresql/blob/master/src/backend/storage/freespace/freespace.c#L545)函数中实现。

#### 2. 查找FSM
&ensp;函数[fsm_search](https://github.com/zhizhengyang/postgresql/blob/master/src/backend/storage/freespace/freespace.c#L632)利用FSM文件查找一个具有指定空闲空间的表块。FSM块在树中的位置
```
typedef struct
{
	int			level;			所在层次
	int			logpageno;		/* 该块所在层次中的第logpageno个FSM块 */
} FSMAddress;
```
可以通过[fsm_logical_to_physical](https://github.com/zhizhengyang/postgresql/blob/master/src/backend/storage/freespace/freespace.c#L403)从FSMAddress中得到物理地址
fsm_search查找的思想非常简单,将其看成查找最大堆二叉树的过程。

#### 3. 调整FSM
&ensp;当从FSM文件中找到了一个具有合适空闲空间的表块并使用它进行了数据插入后，需要调整FSM文件中相关信息进行修改，调整该表块的空闲空间值，同时对其所属的FSM块内的最大二叉树堆进行调整。调整过程由函数[fsm_set_avail](https://github.com/zhizhengyang/postgresql/blob/master/src/backend/storage/freespace/freespace.c#L182)进行

### 3.2.5 可见性映射表
&ensp;为了能加快VACUUM查找包含无效元组的文件块的过程，在pgsql中定义了新的附属文件---可见性映射表(VM)。VM中为表的每个文件块设置了一位，用来标记该文件块是否存在无效元组。对包含无效元组的文件块，VACUUM有两种处理，快速清理（Lazy VACUUM)和完全清理（Full VACUUM）。VM仅在快速清理中会被使用。
&ensp;对于每个表文件，其对应的VM文件命名为:"关系表OID_vm"对该文件的操作在[visibilitymap.c](https://github.com/zhizhengyang/postgresql/blob/a4cae1310bbcfc298c601e5495a1a45203088889/src/backend/access/heap/visibilitymap.c)中进行了定义。
VM文件块结构如下，当表块中所有的元组对当前事务都可见的时候，表块对应的位才为1.

| Column 1  | Column 2  | Column 3  | Column 4 | Column 5 | Column 5 |
|:----------|:----------|:----------|:----------|:----------|:----------|
| PageHeaderData    | bit    | bit    |bit    |...|bit|bit|

### 3.2.6 大数据存储
&ensp;在pgsql中提供了两种大数据存储方式：第一种是TOAST机制，使用数据压缩和线外存储来实现；第二种是大对象机制，使用一个专门的系统表来存储大对象数据。

#### 1. TOAST
&ensp;TOAST(The Oversized-Attribute Storage Technique,超尺寸字段存储技巧)
1） TOAST实现原理
&ensp;在pgsql中只有一部分数据类型支持TOAST，这些数据类型必须有变长的形式，比如TEXT类型。只有在准备存储超过BLOCKSZ/4字节的（通常为2KB）数据时，TOAST机制才会被触发。TOAST机制会试图压缩或线外存储数据（把数据存储在其他的表当中），直到数据比BLOCKSZ/4字节短或者无法得到更好的结果才停止。
&ensp;线外存储的数据会被保存在成为TOAST表的普通表中。如果一个表中有任何一个属性是可以TOAST的，那么该表将有一个关联的TOAST表，其OID存储在表的基本信息的reltoastrelid属性里。如果没有关联的TOAST表，则reltoastrelid属性值为0。
&ensp;TOAST机制识别四种不同的存储数据的策略：
> - PLAIN:避免压缩或者线外存储。
> - EXTENDED：允许压缩和线外存储
> - EXTERNAL:允许线外存储，但是不允许压缩
> - MAIN：允许压缩，但不允许线外存储。

2）TOAST表结构
&ensp;如果采用线外存储方式处理TOAST数据，则线外数据会被分割成不超过TOAST_MAX_CHUNK_SIZE字节的片段(chunck)，每个片段都作为独立的元组存储在相关联的TOAST表中。每个TOAST表的属性都相同。如下

| 属性名称  | 类型  | 描述  |
|:----------|:----------|:----------|
| chunk_id   | oid    | 线外存储时为整个TOAST数据分配的OID   |
| chunk_seq    | int4   | 一个序列号，存储该片段在整个TOAST数据中的位置    |
| chunk_data   | bytea    | 该片段实际的数据    |

&ensp;一个线外存储的数据可以通过一个存储在TOAST属性中的TOAST指针找到。该指针数据结构如下

```
struct varatt_external
{
	int32		va_rawsize;		/* Original data size (includes header) */
	int32		va_extsize;		/* External saved size (doesn't) */
	Oid			va_valueid;		/* Unique ID of value within TOAST table 线外存储数据的OID*/
	Oid			va_toastrelid;	/* RelID of TOAST table containing it TOAST表的OID*/
};


```

3）TOAST操作
&ensp;TOAST机制的实现代码在[tuptoaster.c](https://github.com/zhizhengyang/postgresql/blob/a4cae1310bbcfc298c601e5495a1a45203088889/src/backend/access/heap/tuptoaster.c)
与TOAST相关的主要操作
![toast.jpg](https://s1.ax1x.com/2020/06/21/N1qLZ9.jpg)

2. 大对象
&ensp;PostgreSQL的大对象存储机制可以支持三种数据类型的存储：</br>
1）二进制大对象（BLOB）：主要用来保存非传统数据，如图片、视频、混合媒体等</br>
2）字符大对象（CLOB）：存储大的单字节字符集数据，如文档等。</br>
3）双字节字符大对象（DBCLOB）：用于存储大的双字节字符集数据，如变长双字节字符图形字符串。</br>

(1)大对象存储
&ensp;在pgsql中，所有的大对象都存储在一个叫pg_largeobject的系统表中，其在数据库中的OID固定为2613.

| 属性名称  | 类型 | 描述  |
|:----------|:----------|:----------|
| loid    | oid   | 包含本页的大对象的标识符   |
| pageno    | int4   | 本页在其大对象数据中的页码从零开始计算   |
| data    | bytea   | 存储在大对象中的实际数据  |

&ensp;一个大对象是使用其创建时分配的OID标识的，即OID是pgsql数据库识别大对象的唯一标记。一个大对象将会被分成若干元组存放在系统表pg_largeobject中，每一个元组也称为一个页面。一个元组的大小为2KB。通常一个20M以上的文件，会被存储为1万多个元组。这里将一个元组大小设置为2KB主要是出于两个目的：第一是更新某个元组时不会浪费太大空间，第二是可以触发TOAST的压缩机制。


(2) 大对象管理
创建大对象时，需要创建大对象描述符，大对象描述符结构如下

```
typedef struct LargeObjectDesc
{
	Oid			id;				/* LO's identifier */
	Snapshot	snapshot;		/* snapshot to use */
	SubTransactionId subid;		/* owning subtransaction ID */
	uint32		offset;			/* current seek pointer */
	int			flags;			/* locking info, etc */

/* flag bits: */
#define IFS_RDLOCK		(1 << 0)
#define IFS_WRLOCK		(1 << 1)

} LargeObjectDesc;

```

## 内存管理
&ensp;pgsql的内存管理分为共享内存和本地内存的管理
![readaccessmemory.jpg](https://s1.ax1x.com/2020/06/21/N3y4AJ.jpg)

### 3.3.1 内存上下文概述
&ensp;系统中的内存分配操作在各种语义的内存上下文中进行，所有在内存上下文中分配的内存空间都通过内存上下文进行记录。一个内存上下文就相当于一个进程环境。
1. MemoryContext</br>
&ensp;pgsql的每一个子进程都拥有多个私有的内存上下文，每个子进程的内存上下文组成一个树形结构，每个子结点都用于不同的功能模块，如CacheMemoryContext用于管理Cache、ErrorMemoryContext用于错误处理，每个子结点又可以有自己的子结点。
![memoorycontextstruct.jpg](https://s1.ax1x.com/2020/06/21/N32R2R.jpg)
&ensp;通过树形结构可以跟踪进程中内存上下文的创建和使用情况，当创建一个新的内存上下文时，将其添到某个已存在的内存上下文下面作为其子结点。在清除内存时从根结点开始遍历内存上下文树可以将其所有结点占用的内存上下文完全释放。</br>
&ensp;每个内存上下文中都定义了这个内存上下文所占用内存块的具体位置、大小等相关信息以及其他内存上下文之间的关联信息，只要能获得这个内存上下文，就可以获得其子结点的内存使用状态。</br>
&ensp;在数据结构3.8中显示了每个内存上下文的结点信息。
```
typedef struct MemoryContextData
{
	NodeTag		type;			/* identifies exact kind of context */
	MemoryContextMethods *methods;		/* virtual function table */
	MemoryContext parent;		/* NULL if no parent (toplevel context) */
	MemoryContext firstchild;	/* head of linked list of children */
	MemoryContext nextchild;	/* next child of same parent */
	char	   *name;			/* context name (just for debugging) */
} MemoryContextData;
```
&ensp;MemoryContext中的methods字段是一个MemoryContextMethods类型，包含了一系列函数指针组合的集合，有对内存上下文进行操作的函数。
```
typedef struct MemoryContextData
{
	NodeTag		type;			/* identifies exact kind of context */
	MemoryContextMethods *methods;		/* virtual function table */
	MemoryContext parent;		/* NULL if no parent (toplevel context) */
	MemoryContext firstchild;	/* head of linked list of children */
	MemoryContext nextchild;	/* next child of same parent */
	char	   *name;			/* context name (just for debugging) */
} MemoryContextData;
```
&ensp;全局变量AllocSetMethods中指定了AllocSetContext实现的操作函数，它们一一对应MemoryContextMethods中的操作函数：AllocSetAlloc、AllocSetFree、AllocSetRealloc等。对于内存上下文的操作都是通过这些函数实现的。</br>
&ensp;在任何时候，都有一个当前的MemoryContext，记录在全局变量CurrentMemoryContext里，进程就在这个内存上下文中调用palloc函数来分配内存。在需要变换内存上下文时，可以使用MemoryContextSwitchTo函数将CurrentMemoryContext指向其他的内存上下文。</br>
&ensp;内存上下文的组织结构如图所示。每个AllocSet结构都对应一个内存上下文，AllocSet所管理的内存区域被分成若干块(block)，内存块用AllocBlockData结构表示。每个内存块内又被分成多个称为内存片的单元。AllocSet结构主要包括以下三部分信息:
(1)头部信息header</br>
(2)内存块链表blocks</br>
(3)FreeList数组(用于维护在内存块中被回收的空闲内存片，这些空闲内存片用于再分配)

2. 内存上下文初始化与创建</br>
任何一个pgsql进程在使用内存上下文之前，都需要先进行初始化。内存上下文的初始化工作由函数[MemoryContextInit](https://github.com/zhizhengyang/postgresql/blob/a4cae1310bbcfc298c601e5495a1a45203088889/src/backend/utils/mmgr/mcxt.c#L75)函数来完成。在初始化时首先创建所有内存上下文的根结点TopMemoryContext，然后在该节点下创建子节点ErrorContext用于错误恢复处理:
> - TopMemoryContext:该节点在分配后将一直存在，直到系统退出时候才释放。在该节点下面分配其他内存上下文节点本身所占用的空间。
> - ErrorContext：该节点是TopMemoryContext的第一个子节点，是错误恢复处理的永久性内存上下文，恢复完毕就会进行重置。

&ensp;当初始化完毕后，建立根节点和错误恢复子结点后，pgsql进程可以开始创建其他的内存上下文。内存上下文的创建由[AllocSetContextCreate](https://github.com/zhizhengyang/postgresql/blob/a4cae1310bbcfc298c601e5495a1a45203088889/src/backend/utils/mmgr/aset.c#L326)函数来实现，主要有两个工作：创建内存上下文节点以及分配内存块。
&ensp;内存上下文节点创建由[MemoryContextCreate](https://github.com/zhizhengyang/postgresql/blob/a4cae1310bbcfc298c601e5495a1a45203088889/src/backend/utils/mmgr/mcxt.c)函数完成。

3. 内存上下文中内存的分配</br>
在pgsql中，内存的分配、重分配和释放都使用了自己实现的palloc、repalloc和pfree分别实现内存上下文中的分配、重分配和释放。</br>
函数[AllocSetAlloc](https://github.com/zhizhengyang/postgresql/blob/a4cae1310bbcfc298c601e5495a1a45203088889/src/backend/utils/mmgr/aset.c#L538)负责处理具体的内存分配工作，该函数的参数为一个内存上下文节点以及需要申请的内存大小。

4. 内存上下文中的内存重分配</br>
内存重分配由AllocSetRelloc函数实现。[AllocSetRealloc](https://github.com/zhizhengyang/postgresql/blob/a4cae1310bbcfc298c601e5495a1a45203088889/src/backend/utils/mmgr/aset.c#L879)将在指定的内存上下文中对参数pointer指向的内存空间进行重新分配，新分配的内存大小由size指定。pointer所指向的内存中的内容将被复制到新的内存中，并释放pointer指向的内存空间。AllocSetRealloc函数的返回值就是指向新内存空间的指针。

5. 释放内存上下文</br>
(1)释放一个内存上下文中指定的内存片</br>
当释放一个内存上下文中制定的内存片时，调用函数AllocSetFree。该函数的执行如下：<br>
1）如果指定要释放的内存片是内存块中唯一的内存片，则将该内存块直接释放。</br>
2）否则，将指定的内存片加入到Freelist链表中以便再分配</br>

(2)重置内存上下文
重置内存上下文的工作由函数[AllocSetReset](https://github.com/zhizhengyang/postgresql/blob/a4cae1310bbcfc298c601e5495a1a45203088889/src/backend/utils/mmgr/aset.c#L436)完成。在进行重置时，内存上下文中除了在keeper字段中要保留的内存块外，其他内存块全部释放，包括空闲链表中的内存。keeper中指定保留的内存块将被清空内容，它使得内存上下文重置之后就立刻有一块内存可供使用。

(3) 释放当前内存上下文中的全部内存块
这个工作由[AllocSetDelete](https://github.com/zhizhengyang/postgresql/blob/a4cae1310bbcfc298c601e5495a1a45203088889/src/backend/utils/mmgr/aset.c#L503)

### 3.3.2 高速缓存
&ensp;当数据库访问表时，需要表的模式信息，比如表的列属性、OID、统计信息等。pgsql将表的模式信息存在系统表中，因此要访问表，就需要首先在系统表中取得表的模式信息。pgsql系统中对系统表和普通表模式的访问时是十分频繁的。为了提高这些访问的效率，pgsql设计了高速缓存（cache）来提高访问效率。Cache中包括一个系统表元组（SysCache）和一个表模式信息（RelCache）。SysCache中存放的是最近使用过的系统表的元组，而RelCache中包含所有最近访问过的表模式信息。RelCache中存放的不是元组，而是RelationData数据结构，每个Relation数据结构表示一个表的模式信息，这些信息都由系统表元组中的信息构造而来。

```
typedef struct RelationData
{
	RelFileNode rd_node;		/* relation physical identifier */
	/* use "struct" here to avoid needing to include smgr.h: */
	struct SMgrRelationData *rd_smgr;	/* cached file handle, or NULL */
	BlockNumber rd_targblock;	/* current insertion target block, or
								 * InvalidBlockNumber */
	int			rd_refcnt;		/* reference count */
	bool		rd_istemp;		/* rel is a temporary relation */
	bool		rd_islocaltemp; /* rel is a temp rel of this session */
	bool		rd_isnailed;	/* rel is nailed in cache */
	bool		rd_isvalid;		/* relcache entry is valid */
	char		rd_indexvalid;	/* state of rd_indexlist: 0 = not valid, 1 =
								 * valid, 2 = temporarily forced */
	SubTransactionId rd_createSubid;	/* rel was created in current xact */
	SubTransactionId rd_newRelfilenodeSubid;	/* new relfilenode assigned in
												 * current xact */

	/*
	 * rd_createSubid is the ID of the highest subtransaction the rel has
	 * survived into; or zero if the rel was not created in the current top
	 * transaction.  This should be relied on only for optimization purposes;
	 * it is possible for new-ness to be "forgotten" (eg, after CLUSTER).
	 * Likewise, rd_newRelfilenodeSubid is the ID of the highest
	 * subtransaction the relfilenode change has survived into, or zero if not
	 * changed in the current transaction (or we have forgotten changing it).
	 */
	Form_pg_class rd_rel;		/* RELATION tuple */
	TupleDesc	rd_att;			/* tuple descriptor */
	Oid			rd_id;			/* relation's object id */
	List	   *rd_indexlist;	/* list of OIDs of indexes on relation */
	Bitmapset  *rd_indexattr;	/* identifies columns used in indexes */
	Oid			rd_oidindex;	/* OID of unique index on OID, if any */
	LockInfoData rd_lockInfo;	/* lock mgr's info for locking relation */
	RuleLock   *rd_rules;		/* rewrite rules */
	MemoryContext rd_rulescxt;	/* private memory cxt for rd_rules, if any */
	TriggerDesc *trigdesc;		/* Trigger info, or NULL if rel has none */

	/*
	 * rd_options is set whenever rd_rel is loaded into the relcache entry.
	 * Note that you can NOT look into rd_rel for this data.  NULL means "use
	 * defaults".
	 */
	bytea	   *rd_options;		/* parsed pg_class.reloptions */

	/* These are non-NULL only for an index relation: */
	Form_pg_index rd_index;		/* pg_index tuple describing this index */
	struct HeapTupleData *rd_indextuple;		/* all of pg_index tuple */
	/* "struct HeapTupleData *" avoids need to include htup.h here	*/
	Form_pg_am	rd_am;			/* pg_am tuple for index's AM */

	/*
	 * index access support info (used only for an index relation)
	 *
	 * Note: only default operators and support procs for each opclass are
	 * cached, namely those with lefttype and righttype equal to the opclass's
	 * opcintype.  The arrays are indexed by strategy or support number, which
	 * is a sufficient identifier given that restriction.
	 *
	 * Note: rd_amcache is available for index AMs to cache private data about
	 * an index.  This must be just a cache since it may get reset at any time
	 * (in particular, it will get reset by a relcache inval message for the
	 * index).	If used, it must point to a single memory chunk palloc'd in
	 * rd_indexcxt.  A relcache reset will include freeing that chunk and
	 * setting rd_amcache = NULL.
	 */
	MemoryContext rd_indexcxt;	/* private memory cxt for this stuff */
	RelationAmInfo *rd_aminfo;	/* lookup info for funcs found in pg_am */
	Oid		   *rd_opfamily;	/* OIDs of op families for each index col */
	Oid		   *rd_opcintype;	/* OIDs of opclass declared input data types */
	Oid		   *rd_operator;	/* OIDs of index operators */
	RegProcedure *rd_support;	/* OIDs of support procedures */
	FmgrInfo   *rd_supportinfo; /* lookup info for support procedures */
	int16	   *rd_indoption;	/* per-column AM-specific flags */
	List	   *rd_indexprs;	/* index expression trees, if any */
	List	   *rd_indpred;		/* index predicate tree, if any */
	void	   *rd_amcache;		/* available for use by index AM */

	/*
	 * sizes of the free space and visibility map forks, or InvalidBlockNumber
	 * if not known yet
	 */
	BlockNumber rd_fsm_nblocks;
	BlockNumber rd_vm_nblocks;

	/* use "struct" here to avoid needing to include pgstat.h: */
	struct PgStat_TableStatus *pgstat_info;		/* statistics collection area */
} RelationData;
```

1. SysCache
&ensp;SysCache主要用于缓存系统表元组。SysCache实际是用一个数组实现，数组的长度为预定义的系统表的个数。在pgsql中实现了54个系统表，每个元素的数据结构为CatCache。每个CatCache都有若干个查找关键字，这些关键字及其组合可以用来在CatCache中查找系统表元组。
```

typedef struct catcache
{
	int			id;				/* cache identifier --- see syscache.h */
	struct catcache *cc_next;	/* link to next catcache */
	const char *cc_relname;		/* name of relation the tuples come from */
	Oid			cc_reloid;		/* OID of relation the tuples come from */
	Oid			cc_indexoid;	/* OID of index matching cache keys */
	bool		cc_relisshared; /* is relation shared across databases? */
	TupleDesc	cc_tupdesc;		/* tuple descriptor (copied from reldesc) */
	int			cc_reloidattr;	/* AttrNumber of relation OID attr, or 0 */
	int			cc_ntup;		/* # of tuples currently in this cache */
	int			cc_nbuckets;	/* # of hash buckets in this cache */
	int			cc_nkeys;		/* # of keys (1..4) */
	int			cc_key[4];		/* AttrNumber of each key */
	PGFunction	cc_hashfunc[4]; /* hash function to use for each key */
	ScanKeyData cc_skey[4];		/* precomputed key info for heap scans */
	bool		cc_isname[4];	/* flag key columns that are NAMEs */
	Dllist		cc_lists;		/* list of CatCList structs */
#ifdef CATCACHE_STATS
	long		cc_searches;	/* total # searches against this cache */
	long		cc_hits;		/* # of matches against existing entry */
	long		cc_neg_hits;	/* # of matches against negative entry */
	long		cc_newloads;	/* # of successful loads of new entry */

	/*
	 * cc_searches - (cc_hits + cc_neg_hits + cc_newloads) is number of failed
	 * searches, each of which will result in loading a negative entry
	 */
	long		cc_invals;		/* # of entries invalidated from cache */
	long		cc_lsearches;	/* total # list-searches */
	long		cc_lhits;		/* # of matches against existing lists */
#endif
	Dllist		cc_bucket[1];	/* hash buckets --- VARIABLE LENGTH ARRAY */
} CatCache;
```
2. RelCach
略
3. Cache同步

### 3.3.3 缓冲池管理
&ensp;如果需要访问的系统表元组在Cache中无法找到或者需要访问普通表的元组，就需要对缓冲池进行访问。任何对于表、元组、索引表等的操作都在缓冲池中进行，缓冲池的数据调度都以磁盘块为单位，需要访问的数据以磁盘块为单位调用smgrread函数写入缓冲池，而smgwrite函数将缓冲池中数据写回到磁盘。
&ensp;pgsql有两种缓冲池：共享缓冲池和本地缓冲池。共享缓冲池主要用作普通可共享表的操作场所；本地缓冲池则用作仅本地可见的临时表的操作场所
&ensp;对于缓冲池中缓冲区的管理通过两种机制完成：pin和lock。pin是对缓冲区的访问计数。lock机制为缓冲区的并发访问提供了保障，当有进程对缓冲区进行写操作时加EXCLUSIVE锁，读操作时加SHARE锁。
1. 初始化共享缓冲池
&ensp;共享缓冲池处于共享内存区域，在系统启动时候需要对其进行初始化操作，负责着一工作的函数为[InitBufferPool](https://github.com/zhizhengyang/postgresql/blob/a4cae1310bbcfc298c601e5495a1a45203088889/src/backend/storage/buffer/buf_init.c#L83)。在共享缓冲池管理中，使用了一个全局数组BufferDescriptors来管理池中的缓冲区描述符，其数组元素类型为BufferDesc可以认为BufferDescriptros就是共享缓冲池
```
typedef struct sbufdesc
{
	BufferTag	tag;			/* ID of page contained in buffer */
	BufFlags	flags;			/* see bit definitions above */
	uint16		usage_count;	/* usage counter for clock sweep code */
	unsigned	refcount;		/* # of backends holding pins on buffer */
	int			wait_backend_pid;		/* backend PID of pin-count waiter */

	slock_t		buf_hdr_lock;	/* protects the above fields */

	int			buf_id;			/* buffer's index number (from 0) */
	int			freeNext;		/* link in freelist chain */

	LWLockId	io_in_progress_lock;	/* to wait for I/O to complete */
	LWLockId	content_lock;	/* to lock access to buffer contents */
} BufferDesc;
```
![initcachebuffer.jpg](https://s1.ax1x.com/2020/06/23/NtIAbQ.jpg)
2. 共享缓冲区查询
&ensp;在初始化缓冲池阶段系统为其在共享内存中创建了Hash表。缓冲区描述符中的BufferTag结构体包含了一个缓冲块的物理信息，因此具有唯一性。共享缓冲区查询以BufferTag作为索引键，查找Hash表并返回目标缓冲区在缓冲区描述符数组中的位置。
```
typedef struct buftag
{
	RelFileNode rnode;			/* physical relation identifier */
	ForkNumber	forkNum;
	BlockNumber blockNum;		/* blknum relative to begin of reln */
} BufferTag;

```
&ensp;ReadBuffer_common是所有缓冲区读取的通用函数，定义了本地缓冲区和共享缓冲区的通用读取方法，其流程如下
![readbuffercommon.jpg](https://s1.ax1x.com/2020/06/23/NNr5iF.jpg)
在ReadBuffer_common函数中调用了BufferAlloc函数
![bufferalloc.jpg](https://s1.ax1x.com/2020/06/23/NNynAK.jpg)

3. 共享缓冲区替换策略
&ensp;在共享缓冲池中，初始化定义的缓冲区个数是有限的，并且这个值在初始化分配后将不会再被改变。
&ensp;对于缓冲区的替换策略，在pgsql8.4.1中有两个方案：一般的缓冲区替换策略以及缓冲环替换策略
(1) 一般的缓冲区替换策略
&ensp;首先在缓冲池维持一个FreeList链表，FreeList是一个单向链表。FreeList中的缓冲区通过其描述符中的freeNext字段链接起来，在BufferStrategyControl结构中记录了FreeList第一个和最后一个元素。在某缓冲区refcount变为0时，将其加入到链尾。当需要一个空闲缓冲区用于替换时，则从链首取得。若FreeList中某缓冲区被使用，则从FreeList中删除。
```
typedef struct
{
	/* Clock sweep hand: index of next buffer to consider grabbing */
	int			nextVictimBuffer;

	int			firstFreeBuffer;	/* Head of list of unused buffers */
	int			lastFreeBuffer; /* Tail of list of unused buffers */

	/*
	 * NOTE: lastFreeBuffer is undefined when firstFreeBuffer is -1 (that is,
	 * when the list is empty)
	 */

	/*
	 * Statistics.	These counters should be wide enough that they can't
	 * overflow during a single bgwriter cycle.
	 */
	uint32		completePasses; /* Complete cycles of the clock sweep */
	uint32		numBufferAllocs;	/* Buffers allocated since last reset */
} BufferStrategyControl;
```
(2) 缓冲区替换策略
&ensp;缓冲环策略时缓冲区策略的一种特殊情况，即对于某些请求而言，加载到内存中的磁盘块在使用过后将不会再次被使用，但其又将使用到大量的缓冲区。因此会造成内存中充斥着大量仅需要使用一次的缓冲区。pgsql采用了缓冲环策略，即对于这类请求，为其分配固定数量的缓冲区，一旦用完后，就从环的头部开始重复使用缓冲区。</br>
&ensp;在缓冲环策略下，pgsql会将用过的缓冲区组织成一个链表，首尾相连形成一个环状结构，称之为缓冲环。例如，对于一次连续的只读扫描，在扫描过后，驻留在内存中的缓冲区将不会再被使用到，因此使用过256KB的缓冲池空间就足够。而对于一次VACUUM操作，同样的也使用256KB的缓冲池空间，但是清理过程中产生的脏页并不是从环中删除，而是刷新（WAL）以允许缓冲区被再次使用。
```
typedef struct BufferAccessStrategyData
{
	/* Overall strategy type */
	BufferAccessStrategyType btype;
	/* Number of elements in buffers[] array */
	int			ring_size;

	/*
	 * Index of the "current" slot in the ring, ie, the one most recently
	 * returned by GetBufferFromRing.
	 */
	int			current;

	/*
	 * True if the buffer just returned by StrategyGetBuffer had been in the
	 * ring already.
	 */
	bool		current_was_in_ring;

	/*
	 * Array of buffer numbers.  InvalidBuffer (that is, zero) indicates we
	 * have not yet selected a buffer for this ring slot.  For allocation
	 * simplicity this is palloc'd together with the fixed fields of the
	 * struct.
	 */
	Buffer		buffers[1];		/* VARIABLE SIZE ARRAY */
} BufferAccessStrategyData;
```
4. 本地缓冲池管理
&ensp;本地缓冲池是每个进程所特有的，在进程初始化时自行创建。本地缓冲池对于其他进程是不可见的。它只在创建临时表或操作的数据对其他进程不可见的特殊情况下使用。
&ensp;本地缓冲池的初始化调用InitLocalBuffer来完成。于共享缓冲池不同的是，初始化过程中并不预先创建缓冲区，而是在使用时创建。</br>
1)分配内存并初始化缓冲区描述符数组，默认大小为1000。
2)创建Hash表用于查询本地缓冲区，使用hash_create函数创建本地Hash表，指定Hash函数为tag_hash,该Hash表用全局变量LocalBufHash保存。
![LocalBufferAlloc](https://s1.ax1x.com/2020/06/23/NN7yAe.jpg)

### 3.3.4 IPC
&ensp;pgsql只考虑一个计算机情况下的进程通信。在pgsql中的IPC主要采用共享内存的方式来实现，在系统中开辟一片所有进程都可以读写的内存空间，并约定好进程读写这片内存的时机和方式，这样进程之间就可以通过这片共享的内存来交换数据。在Pgsql的IPC机制提供了以下功能:
1）进程和pgsql的通信机制。
2）统一管理进程的相关变量和函数
3）提供了SI Message机制，即无效消息传递机制。
4）有关清除函数。
